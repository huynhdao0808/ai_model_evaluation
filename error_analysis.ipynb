{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Import functions from error_analysis.py\n",
    "from error_analysis import crop_image, show_image_pairs, generate_confusion_matrices, load_config\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Display configuration for the notebook\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"rtdert_2.0\"\n",
    "dataset_version = \"test1_v1\"\n",
    "# Select the specific run to analyze (use the latest run folder by default)\n",
    "import os\n",
    "from pathlib import Path\n",
    "model_path = f\"prediction/{model}\"\n",
    "# Get all run folders sorted by timestamp (latest first)\n",
    "run_folders = sorted([f for f in os.listdir(model_path) if f.startswith(\"run_\")], reverse=True)\n",
    "if run_folders:\n",
    "    run_folder = run_folders[0]  # Use the latest run by default\n",
    "    print(f\"Using latest run: {run_folder}\")\n",
    "else:\n",
    "    run_folder = \"run_latest\"\n",
    "    print(\"No run folders found. Using default name.\")\n",
    "\n",
    "# Path to the evaluation CSV file\n",
    "result_file = f\"{model_path}/{run_folder}/{model}_evaluation.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv(result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Optimize the confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval[(df_eval['gt']=='impression') & (df_eval['eval']==\"TP\")].sort_values(\"confidence\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval[(df_eval['gt']=='einriss') & (df_eval['eval']==\"TP\")]['confidence'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataframe is named df\n",
    "\n",
    "# Find rows that are duplicates based on 'gt', 'gt_location', and 'filename'\n",
    "duplicates = df_eval.duplicated(subset=['gt', 'gt_location', 'filename'], keep=False)\n",
    "\n",
    "# Separate the dataframe into rows that are duplicates and those that are not\n",
    "df_duplicates = df_eval[duplicates]\n",
    "df_non_duplicates = df_eval[~duplicates]\n",
    "\n",
    "# Drop rows where 'pred' is null in the duplicate rows\n",
    "df_duplicates = df_duplicates.dropna(subset=['pred'])\n",
    "\n",
    "# Combine the non-duplicate rows and the cleaned duplicate rows\n",
    "df_eval = pd.concat([df_non_duplicates, df_duplicates])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import error_analysis\n",
    "reload(error_analysis)\n",
    "\n",
    "# Specify the directory for images\n",
    "directory = f'images/{dataset_version}'\n",
    "\n",
    "# Get the list of file names\n",
    "file_names = os.listdir(directory)\n",
    "\n",
    "# Filter out directories, if needed\n",
    "full_list = [f.split(\".\")[0] for f in file_names if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "classes = [\"impression\", \"einriss\", \"abriss\", \"asperity\", \"ausseinriss\"]\n",
    "\n",
    "output_dir = os.path.join(f\"{model_path}/{run_folder}\", \"analysis\")\n",
    "\n",
    "result = error_analysis.generate_confusion_matrices(df_eval, full_list, classes, output_dir, model, show = True, save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import error_analysis\n",
    "reload(error_analysis)\n",
    "\n",
    "def_names =  [ \"einriss\", \"abriss\", \"ausseinriss\", \"impression\", \"asperity\" ]\n",
    "# def_names =  [ \"abriss\"]\n",
    "type = \"FN\" # FN, FP\n",
    "# modes = [\"merge\", \"wrong\", \"notdetect\"]\n",
    "modes = [\"notdetect\"]\n",
    "\n",
    "for def_name in def_names:\n",
    "    \n",
    "    for mode in modes:\n",
    "\n",
    "        if mode == \"merge\":\n",
    "            fil_df_eval = df_eval[(df_eval['gt']==def_name) & (df_eval['eval']==type) & (df_eval['pred'] == df_eval['gt'])]\n",
    "        elif mode == \"wrong\":\n",
    "            fil_df_eval = df_eval[(df_eval['gt']==def_name) & (df_eval['eval']==type) & (df_eval['pred'] != df_eval['gt']) & df_eval['pred'].notnull()]\n",
    "        elif mode == \"notdetect\":\n",
    "            fil_df_eval = df_eval[(df_eval['gt']==def_name) & (df_eval['eval']==type) & (df_eval['pred'].isnull())]\n",
    "            \n",
    "        if fil_df_eval.shape[0] != 0:\n",
    "            fil_df_eval['gt_path'] = f\"{model_path}/{run_folder}/image_unfilter_crop/\" + fil_df_eval['filename'] + \".bmp\"\n",
    "            fil_df_eval = fil_df_eval.sort_values(\"pred\")\n",
    "            image_pairs = list(zip(fil_df_eval['filename'], fil_df_eval['gt_path'], fil_df_eval['gt'], fil_df_eval['pred']))\n",
    "\n",
    "            # In notebook, we use show_plot=True to display the images interactively\n",
    "            error_analysis.show_image_pairs(image_pairs, mode, def_name, model_path, run_folder, save_images=False, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import error_analysis\n",
    "reload(error_analysis)\n",
    "\n",
    "# def_names =  [ \"einriss\", \"abriss\", \"ausseinriss\", \"impression\", \"asperity\" ]\n",
    "def_names = ['abriss']\n",
    "type = \"FP\" # FN, FP\n",
    "# modes = [\"wrong\", \"redundant\"]\n",
    "modes = [ \"redundant\"]\n",
    "\n",
    "for def_name in def_names:\n",
    "    for mode in modes:\n",
    "\n",
    "        if mode == \"wrong\":\n",
    "            fil_df_eval = df_eval[(df_eval['pred']==def_name) & (df_eval['eval']==type) & (df_eval['pred'] != df_eval['gt']) & df_eval['gt'].notnull()]\n",
    "        elif mode == \"redundant\":\n",
    "            fil_df_eval = df_eval[(df_eval['pred']==def_name) & (df_eval['eval']==type) & (df_eval['gt'].isnull())]\n",
    "            \n",
    "        if fil_df_eval.shape[0] != 0:\n",
    "            fil_df_eval['gt_path'] = f\"{model_path}/{run_folder}/image_unfilter_crop/\" + fil_df_eval['filename'] + \".bmp\"\n",
    "            fil_df_eval = fil_df_eval.sort_values(\"gt\")\n",
    "            image_pairs = list(zip(fil_df_eval['filename'], fil_df_eval['gt_path'], fil_df_eval['pred'], fil_df_eval['gt']))\n",
    "\n",
    "            # In notebook, we use show_plot=True to display the images interactively\n",
    "            error_analysis.show_image_pairs(image_pairs, mode, def_name, model_path, run_folder, None, save_images=False, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import error_analysis\n",
    "reload(error_analysis)\n",
    "\n",
    "# def_names =  [ \"einriss\", \"abriss\", \"ausseinriss\", \"impression\", \"asperity\" ]\n",
    "def_names = ['abriss']\n",
    "type = \"TP\"\n",
    "modes = [\"TP\"]\n",
    "\n",
    "for def_name in def_names:\n",
    "    for mode in modes:\n",
    "\n",
    "        fil_df_eval = df_eval[(df_eval['pred']==def_name) & (df_eval['eval']==type)]\n",
    "            \n",
    "        if fil_df_eval.shape[0] != 0:\n",
    "            fil_df_eval['gt_path'] = f\"{model_path}/{run_folder}/image_unfilter_crop/\" + fil_df_eval['filename'] + \".bmp\"\n",
    "            fil_df_eval = fil_df_eval.sort_values(\"gt\")\n",
    "            image_pairs = list(zip(fil_df_eval['filename'], fil_df_eval['gt_path'], fil_df_eval['pred'], fil_df_eval['gt']))\n",
    "\n",
    "            # In notebook, we use show_plot=True to display the images interactively\n",
    "            error_analysis.show_image_pairs(image_pairs, mode, def_name, model_path, run_folder, None, save_images=False, show_plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
